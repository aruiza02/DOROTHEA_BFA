{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este cuaderno se va a presentar el apartado en el que el proyecto comienza a utilizar las librerías python de pandas y también el uso de Scikit-Learn:\n",
    "\n",
    "##### Normalización del documento\n",
    "* 1.- Cargar los archivos .csv de las pruebas recogidas, tanto benignas como malignas.\n",
    "* 2.- Realizar la unión de ambos archivos en uno solo, añadiendo una columna final con valores \"0\" para tráfico benigno y \"1\" para tráfico maligno\n",
    "* 3.- Eliminar columnas innecesarias que producen ruido\n",
    "* 4.- Completar el archivo final para el estudio realizando la normalización usando MinMaxScaler()\n",
    "\n",
    "##### Aprendizaje y resultados de precisión\n",
    "* 5.- Importación de librerias\n",
    "* 6.- Se asignan archivos de trabajo y se divide el conjunto de datos\n",
    "* 7.- Se crea y entrena el modelo\n",
    "* 8.- Se muestra la precisión del modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización del documento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la importación de librerías en primera instancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import ipaddress\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, classification_report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar archivos\n",
    "\n",
    "En este primer paso, se procede a cargar los archivos generados tras la ejecución de Dorothea tanto con la opción \"-t attack\" como con \"-t normal\"\n",
    "\n",
    "Se generan una serie de interacciones en el sistemas que permiten generar archivos .csv con todo el flujo de datos de red necesario para el estudio.\n",
    "\n",
    "Tras generar dichos .csv, estos se cargan de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo CSV\n",
    "maligno = \"/home/osboxes/Documents/Dorothea/Labs/lab_attacks/results/Pruebas/Maligno.csv\"\n",
    "benigno = \"/home/osboxes/Documents/Dorothea/Labs/lab_normal/results/Benigno.csv\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificación y unión de archivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se agregan los valores \"0\" para tráfico benigno y \"1\" para tráfico maligno en los archivos .csv anteriormente indicados. Para ello, en primer lugar se crean dos variables con dichos valores para posteriormente agregarlos a los archivos .csv cargados en una nueva columna denominada \"Tipo Trafico\". Se finaliza este paso unificando ambos archivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor fijo a añadir en la nueva columna\n",
    "valor_maligno = 1\n",
    "valor_benigno = 0\n",
    "\n",
    "# Cargar archivos CSV\n",
    "df1 = pd.read_csv(maligno)\n",
    "df2 = pd.read_csv(benigno)\n",
    "dftest = pd.read_csv(test)\n",
    "\n",
    "# Agregar una columna con un valor fijo\n",
    "df1['Tipo Trafico'] = valor_maligno\n",
    "df2['Tipo Trafico'] = valor_benigno\n",
    "\n",
    "# Se unen los archivos en uno\n",
    "df_concatenado = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de columnas innecesarias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una mejor gestión de los datos y una mejor comprensión, se proceden a eliminar las columnas que introducen ruido en las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan columnas innecesarias\n",
    "columnas_eliminar = ['#:unix_secs', 'unix_nsecs', 'sysuptime', 'exaddr', 'first', 'last', 'engine_type', 'engine_id',  'nexthop', 'src_mask', 'dst_mask', 'src_as', 'dst_as' ]  \n",
    "df_concatenado = df_concatenado.drop(columnas_eliminar, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestión de las columnas que contienen direcciones IP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para normalizar las columnas del archivo .csv que tienen formato de dirección IP, en primer lugar se transforman dichas columnas a un formato con el cual se pueda trabajar una vez que se aplique la normalización posterior.\n",
    "En este caso se procede con la transformación a enteros utilizando int(ipaddress.IPv4Address(ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_to_integer(ip):\n",
    "\treturn int(ipaddress.IPv4Address(ip))\n",
    "\n",
    "# Se definen las columnas del archivo que tienen las direcciones IP que se tienen que normalizar\n",
    "DireccionesIP = [\"srcaddr\", \"dstaddr\"]\n",
    "for direccion in DireccionesIP:\n",
    "    df_concatenado[direccion] = df_concatenado[direccion].apply(ip_to_integer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivo normalizado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar este primer apartado, se procede con la normalización del documento a través de la función MinMaxScaler() facilitada por la libreria Scikit-Learn.\n",
    "De esta manera, se realizará el entrenamiento de nuestra inteligencia con valores entre 0 y 1, permitiendo así, que las pruebas realizadas no sufran errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza ahora la normalización se las columnas para \n",
    "# posteriormente trabajar con ellas\n",
    "columnas = ['dpkts', 'doctets', 'srcaddr', 'dstaddr', 'input', 'output', 'srcport', 'dstport', 'prot', 'tos', 'tcp_flags', 'Tipo Trafico']\n",
    "scaler = preprocessing.Normalizer(norm='l2', copy=True)\n",
    "scaler = scaler.fit(df_concatenado)\n",
    "scaler = MinMaxScaler()\n",
    "df_concatenado[columnas] = scaler.fit_transform(df_concatenado[columnas])\n",
    "df_concatenado.to_csv('/home/osboxes/Documents/Dorothea/Scripts/csv_normalizado.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje y resultados de precisión"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En segunda instancia se trabaja con algortimos de aprendizaje automática e inteligencia artificial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se asigna el archivo .csv, se facilitan los datos para el estudio y se divide el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_concatenado.drop('Tipo Trafico', axis=1)  # Coge todos los datos menos los de la columna Tipo Trafico\n",
    "y = df_concatenado['Tipo Trafico'] #Valor para el estudio\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, se crea y entrena el modelo MLP para posteriomente evaluarlo y obtener los valores objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "# Crear y entrenar el modelo MLP\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000)  # Ajusta los tamaños de las capas ocultas según tus necesidades\n",
    "mlp2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = mlp2.score(X_test, y_test)\n",
    "print('Precisión del modelo:', accuracy)\n",
    "\n",
    "# Realizar predicciones sobre los datos de prueba\n",
    "y_pred = mlp2.predict(X_test)\n",
    "\n",
    "# Obtener la matriz de confusión\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusión:', confusion_mat)\n",
    "\n",
    "# Obtener el informe de clasificación\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print('Classification Report:', classification_rep)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
